{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67241942",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4b1bf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Zainab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Zainab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Zainab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Zainab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original data\n",
    "original_df = pd.read_csv('./dataset/cleaned_music_reviews.csv') \n",
    "original_df['is_synthetic'] = False\n",
    "synth_df = pd.read_csv('./dataset/synthetic_low_ratings_4000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f39abbf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 4)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synth_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "42443b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with 'Cleaned_Review' and 'Rating' from both original and synthetic data\n",
    "cleaned_df = pd.concat([\n",
    "    original_df[['Cleaned_Review', 'Rating']],\n",
    "    synth_df[['Cleaned_Review', 'Rating']]\n",
    "], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2e35272c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cleaned_Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>think actually underrate ok computer anything ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>get radiohead rub lot people wrong way lot peo...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>would like think good letting wider critical w...</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>radiohead devotee like bowie devotee find unex...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wrote shining excellent review album browser w...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97836</th>\n",
       "      <td>forgettable favorite mastering quality lacklus...</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97837</th>\n",
       "      <td>indie release encouraging generic groove demon...</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97838</th>\n",
       "      <td>bad punk album forgettable delivers redeeming ...</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97839</th>\n",
       "      <td>electronic release blooming disappointing demo...</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97840</th>\n",
       "      <td>decent classical albun disappointing promising...</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97841 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Cleaned_Review  Rating\n",
       "0      think actually underrate ok computer anything ...     5.0\n",
       "1      get radiohead rub lot people wrong way lot peo...     5.0\n",
       "2      would like think good letting wider critical w...     4.5\n",
       "3      radiohead devotee like bowie devotee find unex...     4.0\n",
       "4      wrote shining excellent review album browser w...     5.0\n",
       "...                                                  ...     ...\n",
       "97836  forgettable favorite mastering quality lacklus...     2.5\n",
       "97837  indie release encouraging generic groove demon...     2.5\n",
       "97838  bad punk album forgettable delivers redeeming ...     2.5\n",
       "97839  electronic release blooming disappointing demo...     2.5\n",
       "97840  decent classical albun disappointing promising...     2.5\n",
       "\n",
       "[97841 rows x 2 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 97841\n",
      "Rating distribution:\n",
      "Rating\n",
      "5.0    29395\n",
      "4.5    17728\n",
      "4.0    14153\n",
      "3.5     7011\n",
      "2.5     6201\n",
      "2.0     5387\n",
      "1.5     4634\n",
      "1.0     4521\n",
      "3.0     4416\n",
      "0.5     4395\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset size: {len(cleaned_df)}\")\n",
    "print(f\"Rating distribution:\\n{cleaned_df['Rating'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cleaned_Review    0\n",
       "Rating            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d77efd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop null values\n",
    "cleaned_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cleaned_df['Cleaned_Review'] == '').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_csv = cleaned_df.to_csv('./dataset/cleaned_music_reviews2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97841, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "cleaned_df = pd.read_csv('./dataset/cleaned_music_reviews2.csv')\n",
    "cleaned_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95719da7",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1,3),\n",
    "    max_df=0.8,\n",
    "    min_df=5,\n",
    "    sublinear_tf=True \n",
    ")\n",
    "\n",
    "X = tfidf_vectorizer.fit_transform(cleaned_df['Cleaned_Review'])\n",
    "vocab = tfidf_vectorizer.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d19fb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = cleaned_df['Rating']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a93cb9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.38803624804141473\n",
      "R2: 0.799939406465966\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Higher weights for rare ratings)\n",
    "\n",
    "model = Ridge(alpha=1.0)\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test,y_pred)\n",
    "r2 = r2_score(y_test,y_pred)\n",
    "\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"R2: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94ac87a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function which tokenizes the text,lowercase the text, remove stopwords, and lemmatize the text \n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s!?]', '', text)  # Keep !? for sentiment\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = list(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d44854e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Review: The album was a masterpiece from start to finish\n",
      "Predicted rating: 4.98 (scale: 0.5-5.0)\n",
      "\n",
      "Review: This album changed my life! Perfect in every way\n",
      "Predicted rating: 5.00 (scale: 0.5-5.0)\n",
      "\n",
      "Review: Some good tracks but overall disappointing\n",
      "Predicted rating: 3.00 (scale: 0.5-5.0)\n",
      "\n",
      "Review: The vocals were amazing, though the production quality ruined it\n",
      "Predicted rating: 3.19 (scale: 0.5-5.0)\n",
      "\n",
      "Review: A genre-defying record that blends jazz, electronica, and rock seamlessly, though some tracks feel unnecessarily long and meandering.\n",
      "Predicted rating: 3.50 (scale: 0.5-5.0)\n",
      "\n",
      "Review: Despite the hype, the album lacks originality and feels like a rehash of the band's previous work, with only a few standout moments.\n",
      "Predicted rating: 2.64 (scale: 0.5-5.0)\n",
      "\n",
      "Review: The production is lush and detailed, but the lyrics are pretentious and the melodies forgettable, making for a frustrating listen.\n",
      "Predicted rating: 2.85 (scale: 0.5-5.0)\n",
      "\n",
      "Review: There are a few catchy songs, but most of the album is forgettable.\n",
      "Predicted rating: 2.97 (scale: 0.5-5.0)\n",
      "\n",
      "Review: The instrumentation is solid, but the songwriting leaves much to be desired.\n",
      "Predicted rating: 3.58 (scale: 0.5-5.0)\n",
      "\n",
      "Review: Some tracks are fantastic, others are just filler.\n",
      "Predicted rating: 4.42 (scale: 0.5-5.0)\n",
      "\n",
      "Review: Wow, what an album. I totally needed another hour of generic pop songs in my life.\n",
      "Predicted rating: 3.79 (scale: 0.5-5.0)\n",
      "\n",
      "Review: Groundbreaking stuff—I've never heard such originality in a song called 'Love Tonight' before.\n",
      "Predicted rating: 4.28 (scale: 0.5-5.0)\n",
      "\n",
      "Review: If boredom was an art form, this album would be a masterpiece.\n",
      "Predicted rating: 4.08 (scale: 0.5-5.0)\n",
      "\n",
      "Review: Oh great, another autotuned ballad. Just what the world was missing.\n",
      "Predicted rating: 3.65 (scale: 0.5-5.0)\n",
      "\n",
      "Review: Truly inspiring how they managed to make every track sound exactly the same.\n",
      "Predicted rating: 4.33 (scale: 0.5-5.0)\n",
      "\n",
      "Review: I laughed, I cried, mostly because I couldn't believe I paid for this.\n",
      "Predicted rating: 4.44 (scale: 0.5-5.0)\n",
      "\n",
      "Review: This album really redefines the word 'mediocre'.\n",
      "Predicted rating: 2.99 (scale: 0.5-5.0)\n",
      "\n",
      "Review: So innovative, I almost didn't fall asleep halfway through.\n",
      "Predicted rating: 4.35 (scale: 0.5-5.0)\n",
      "\n",
      "Review: Mediocre at best - nothing special\n",
      "Predicted rating: 1.58 (scale: 0.5-5.0)\n",
      "\n",
      "Review: This album was the worst thing I heard in my life, Death to the artist and the producer, disgusting, awful, bad , waste of time\n",
      "Predicted rating: 1.72 (scale: 0.5-5.0)\n",
      "\n",
      "Review: Absolutely terrible album, not a single redeeming quality. I regret listening to it.\n",
      "Predicted rating: 2.02 (scale: 0.5-5.0)\n",
      "\n",
      "Review: Horrible in every way, the worst music I've ever heard.\n",
      "Predicted rating: 2.97 (scale: 0.5-5.0)\n",
      "\n",
      "Review: Unbearable noise, couldn't finish a single track.\n",
      "Predicted rating: 3.42 (scale: 0.5-5.0)\n",
      "\n",
      "Review: A complete disaster, avoid at all costs.\n",
      "Predicted rating: 0.50 (scale: 0.5-5.0)\n",
      "\n",
      "Review: Painful to listen to, a total waste of time.\n",
      "Predicted rating: 2.41 (scale: 0.5-5.0)\n",
      "\n",
      "Review: Zero talent, zero effort, zero enjoyment.\n",
      "Predicted rating: 2.82 (scale: 0.5-5.0)\n",
      "\n",
      "Review: This album is an insult to music.\n",
      "Predicted rating: 4.46 (scale: 0.5-5.0)\n",
      "\n",
      "Review: If I could give it a zero, I would.\n",
      "Predicted rating: 3.76 (scale: 0.5-5.0)\n",
      "\n",
      "Review: The most disappointing and awful release of the year.\n",
      "Predicted rating: 1.29 (scale: 0.5-5.0)\n",
      "\n",
      "Review: Disgusting, offensive, and unlistenable.\n",
      "Predicted rating: 2.11 (scale: 0.5-5.0)\n"
     ]
    }
   ],
   "source": [
    "def predict_rating(review):\n",
    "    # Preprocess\n",
    "    processed_review = preprocess_text(review)\n",
    "\n",
    "    # Transform\n",
    "    review_vector = tfidf_vectorizer.transform([processed_review])\n",
    "    # Predict\n",
    "    rating = model.predict(review_vector)[0]\n",
    "\n",
    "    # Ensure rating is within original bounds\n",
    "    min_rating, max_rating = cleaned_df['Rating'].min(), cleaned_df['Rating'].max()\n",
    "    rating = np.clip(rating, min_rating, max_rating)\n",
    "\n",
    "    return f\"Predicted rating: {rating:.2f} (scale: {min_rating}-{max_rating})\"\n",
    "\n",
    "# Test cases\n",
    "test_reviews = [\n",
    "    # Positive\n",
    "    \"The album was a masterpiece from start to finish\",\n",
    "    \"This album changed my life! Perfect in every way\",\n",
    "    # Mixed\n",
    "    \"Some good tracks but overall disappointing\",\n",
    "    \"The vocals were amazing, though the production quality ruined it\",\n",
    "    \"A genre-defying record that blends jazz, electronica, and rock seamlessly, though some tracks feel unnecessarily long and meandering.\",\n",
    "    \"Despite the hype, the album lacks originality and feels like a rehash of the band's previous work, with only a few standout moments.\",\n",
    "    \"The production is lush and detailed, but the lyrics are pretentious and the melodies forgettable, making for a frustrating listen.\",\n",
    "    \"There are a few catchy songs, but most of the album is forgettable.\",\n",
    "    \"The instrumentation is solid, but the songwriting leaves much to be desired.\",\n",
    "    \"Some tracks are fantastic, others are just filler.\",\n",
    "    # Sarcastic\n",
    "    \"Wow, what an album. I totally needed another hour of generic pop songs in my life.\",\n",
    "    \"Groundbreaking stuff—I've never heard such originality in a song called 'Love Tonight' before.\",\n",
    "    \"If boredom was an art form, this album would be a masterpiece.\",\n",
    "    \"Oh great, another autotuned ballad. Just what the world was missing.\",\n",
    "    \"Truly inspiring how they managed to make every track sound exactly the same.\",\n",
    "    \"I laughed, I cried, mostly because I couldn't believe I paid for this.\",\n",
    "    \"This album really redefines the word 'mediocre'.\",\n",
    "    \"So innovative, I almost didn't fall asleep halfway through.\",\n",
    "    # Negative\n",
    "    \"Mediocre at best - nothing special\",\n",
    "    \"This album was the worst thing I heard in my life, Death to the artist and the producer, disgusting, awful, bad , waste of time\",\n",
    "    \"Absolutely terrible album, not a single redeeming quality. I regret listening to it.\",\n",
    "    \"Horrible in every way, the worst music I've ever heard.\",\n",
    "    \"Unbearable noise, couldn't finish a single track.\",\n",
    "    \"A complete disaster, avoid at all costs.\",\n",
    "    \"Painful to listen to, a total waste of time.\",\n",
    "    \"Zero talent, zero effort, zero enjoyment.\",\n",
    "    \"This album is an insult to music.\",\n",
    "    \"If I could give it a zero, I would.\",\n",
    "    \"The most disappointing and awful release of the year.\",\n",
    "    \"Disgusting, offensive, and unlistenable.\"\n",
    "]\n",
    "\n",
    "for review in test_reviews:\n",
    "    print(f\"\\nReview: {review}\")\n",
    "    print(predict_rating(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9154770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted rating for the review 'hi' is: 4.29\n"
     ]
    }
   ],
   "source": [
    "# Get user input\n",
    "text = input(\"Enter review: \")\n",
    "\n",
    "# Clean the review text\n",
    "clean_data = preprocess_text(text)  # This should return a cleaned string\n",
    "\n",
    "# Vectorize using the already trained vectorizer (do NOT use fit_transform)\n",
    "X = tfidf_vectorizer.transform([clean_data])  # Wrap in a list to avoid error\n",
    "\n",
    "# Predict using the trained model\n",
    "predicted_rating = model.predict(X)\n",
    "\n",
    "# Output the result\n",
    "print(f\"The predicted rating for the review '{text}' is: {predicted_rating[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cdfb5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/tfidf.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save trained models\n",
    "joblib.dump(model, 'models/model_ridge.pkl')\n",
    "joblib.dump(tfidf_vectorizer, 'models/tfidf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ba3e34",
   "metadata": {},
   "source": [
    "## Word2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e5da43de",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m tokenized_reviews \u001b[38;5;241m=\u001b[39m [review\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m cleaned_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCleaned_Review\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Train a Word2Vec model\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m w2v_model \u001b[38;5;241m=\u001b[39m Word2Vec(sentences\u001b[38;5;241m=\u001b[39mtokenized_reviews, vector_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, min_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, sg\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Function to get average word2vec embedding for a review\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_review_vector\u001b[39m(tokens, model, vector_size):\n",
      "File \u001b[1;32mc:\\Users\\Zainab\\anaconda3\\Lib\\site-packages\\gensim\\models\\word2vec.py:429\u001b[0m, in \u001b[0;36mWord2Vec.__init__\u001b[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus_iterable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m corpus_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, passes\u001b[38;5;241m=\u001b[39m(epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 429\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_vocab(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, trim_rule\u001b[38;5;241m=\u001b[39mtrim_rule)\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[0;32m    431\u001b[0m         corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, total_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_count,\n\u001b[0;32m    432\u001b[0m         total_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_total_words, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs, start_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha,\n\u001b[0;32m    433\u001b[0m         end_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_alpha, compute_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Zainab\\anaconda3\\Lib\\site-packages\\gensim\\models\\word2vec.py:495\u001b[0m, in \u001b[0;36mWord2Vec.build_vocab\u001b[1;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_count \u001b[38;5;241m=\u001b[39m corpus_count\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_total_words \u001b[38;5;241m=\u001b[39m total_words\n\u001b[1;32m--> 495\u001b[0m report_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_vocab(update\u001b[38;5;241m=\u001b[39mupdate, keep_raw_vocab\u001b[38;5;241m=\u001b[39mkeep_raw_vocab, trim_rule\u001b[38;5;241m=\u001b[39mtrim_rule, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    496\u001b[0m report_values[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmemory\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimate_memory(vocab_size\u001b[38;5;241m=\u001b[39mreport_values[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_retained_words\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_weights(update\u001b[38;5;241m=\u001b[39mupdate)\n",
      "File \u001b[1;32mc:\\Users\\Zainab\\anaconda3\\Lib\\site-packages\\gensim\\models\\word2vec.py:660\u001b[0m, in \u001b[0;36mWord2Vec.prepare_vocab\u001b[1;34m(self, update, keep_raw_vocab, trim_rule, min_count, sample, dry_run)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dry_run:\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;66;03m# now update counts\u001b[39;00m\n\u001b[0;32m    659\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mindex_to_key:\n\u001b[1;32m--> 660\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mset_vecattr(word, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_vocab[word])\n\u001b[0;32m    661\u001b[0m original_unique_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(retain_words) \u001b[38;5;241m+\u001b[39m drop_unique\n\u001b[0;32m    662\u001b[0m retain_unique_pct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(retain_words) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(original_unique_total, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Zainab\\anaconda3\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:353\u001b[0m, in \u001b[0;36mKeyedVectors.set_vecattr\u001b[1;34m(self, key, attr, val)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_vecattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, attr, val):\n\u001b[0;32m    335\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Set attribute associated with the given key to value.\u001b[39;00m\n\u001b[0;32m    336\u001b[0m \n\u001b[0;32m    337\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    351\u001b[0m \n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallocate_vecattrs(attrs\u001b[38;5;241m=\u001b[39m[attr], types\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mtype\u001b[39m(val)])\n\u001b[0;32m    354\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index(key)\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpandos[attr][index] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[1;32mc:\\Users\\Zainab\\anaconda3\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:299\u001b[0m, in \u001b[0;36mKeyedVectors.allocate_vecattrs\u001b[1;34m(self, attrs, types)\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_int\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpandos:\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpandos[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_int\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpandos[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_int\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint32)\n\u001b[1;32m--> 299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mallocate_vecattrs\u001b[39m(\u001b[38;5;28mself\u001b[39m, attrs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    300\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Ensure arrays for given per-vector extra-attribute names & types exist, at right size.\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \n\u001b[0;32m    302\u001b[0m \u001b[38;5;124;03m    The length of the index_to_key list is canonical 'intended size' of KeyedVectors,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    305\u001b[0m \n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;66;03m# with no arguments, adjust lengths of existing vecattr arrays to match length of index_to_key\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "# Tokenize the cleaned reviews for Word2Vec training\n",
    "tokenized_reviews = [review.split() for review in cleaned_df['Cleaned_Review']]\n",
    "\n",
    "# Train a Word2Vec model\n",
    "w2v_model = Word2Vec(sentences=tokenized_reviews, vector_size=100, window=5, min_count=5, workers=4, sg=1, seed=42)\n",
    "\n",
    "# Function to get average word2vec embedding for a review\n",
    "def get_review_vector(tokens, model, vector_size):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(vector_size)\n",
    "    else:\n",
    "        return np.mean(vectors, axis=0)\n",
    "\n",
    "# Create feature matrix using average word2vec embeddings\n",
    "X_w2v = np.vstack([\n",
    "    get_review_vector(tokens, w2v_model, w2v_model.vector_size)\n",
    "    for tokens in tokenized_reviews\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2191bf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use the Word2Vec feature matrix for train-test split\n",
    "X_w2v_train, X_w2v_test, y_w2v_train, y_w2v_test = train_test_split(\n",
    "    X_w2v, y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c97486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Ridge MSE: 0.5594102307113115\n",
      "Word2Vec Ridge R2: 0.7115837931378741\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "ridge_w2v = Ridge(alpha=1.0)\n",
    "ridge_w2v.fit(X_w2v_train, y_w2v_train)\n",
    "y_w2v_pred = ridge_w2v.predict(X_w2v_test)\n",
    "\n",
    "mse_w2v = mean_squared_error(y_w2v_test, y_w2v_pred)\n",
    "r2_w2v = r2_score(y_w2v_test, y_w2v_pred)\n",
    "\n",
    "print(f\"Word2Vec Ridge MSE: {mse_w2v}\")\n",
    "print(f\"Word2Vec Ridge R2: {r2_w2v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12383806",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function which tokenizes the text,lowercase the text, remove stopwords, and lemmatize the text \n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s!?]', '', text)  # Keep !? for sentiment\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = list(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d57dfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Review: The album was a masterpiece from start to finish\n",
      "Predicted rating: 5.00 (scale: 0.5-5.0)\n",
      "\n",
      "Review: This album changed my life! Perfect in every way\n",
      "Predicted rating: 5.00 (scale: 0.5-5.0)\n",
      "\n",
      "Review: Some good tracks but overall disappointing\n",
      "Predicted rating: 2.95 (scale: 0.5-5.0)\n",
      "\n",
      "Review: The vocals were amazing, though the production quality ruined it\n",
      "Predicted rating: 2.64 (scale: 0.5-5.0)\n",
      "\n",
      "Review: A genre-defying record that blends jazz, electronica, and rock seamlessly, though some tracks feel unnecessarily long and meandering.\n",
      "Predicted rating: 3.72 (scale: 0.5-5.0)\n",
      "\n",
      "Review: Despite the hype, the album lacks originality and feels like a rehash of the band's previous work, with only a few standout moments.\n",
      "Predicted rating: 2.84 (scale: 0.5-5.0)\n",
      "\n",
      "Review: The production is lush and detailed, but the lyrics are pretentious and the melodies forgettable, making for a frustrating listen.\n",
      "Predicted rating: 2.79 (scale: 0.5-5.0)\n",
      "\n",
      "Review: There are a few catchy songs, but most of the album is forgettable.\n",
      "Predicted rating: 3.38 (scale: 0.5-5.0)\n",
      "\n",
      "Review: The instrumentation is solid, but the songwriting leaves much to be desired.\n",
      "Predicted rating: 4.31 (scale: 0.5-5.0)\n",
      "\n",
      "Review: Some tracks are fantastic, others are just filler.\n",
      "Predicted rating: 3.86 (scale: 0.5-5.0)\n",
      "\n",
      "Review: Wow, what an album. I totally needed another hour of generic pop songs in my life.\n",
      "Predicted rating: 3.76 (scale: 0.5-5.0)\n",
      "\n",
      "Review: Groundbreaking stuff—I've never heard such originality in a song called 'Love Tonight' before.\n",
      "Predicted rating: 4.49 (scale: 0.5-5.0)\n",
      "\n",
      "Review: If boredom was an art form, this album would be a masterpiece.\n",
      "Predicted rating: 4.98 (scale: 0.5-5.0)\n",
      "\n",
      "Review: Oh great, another autotuned ballad. Just what the world was missing.\n",
      "Predicted rating: 4.29 (scale: 0.5-5.0)\n",
      "\n",
      "Review: Truly inspiring how they managed to make every track sound exactly the same.\n",
      "Predicted rating: 4.39 (scale: 0.5-5.0)\n",
      "\n",
      "Review: I laughed, I cried, mostly because I couldn't believe I paid for this.\n",
      "Predicted rating: 3.37 (scale: 0.5-5.0)\n",
      "\n",
      "Review: This album really redefines the word 'mediocre'.\n",
      "Predicted rating: 3.23 (scale: 0.5-5.0)\n",
      "\n",
      "Review: So innovative, I almost didn't fall asleep halfway through.\n",
      "Predicted rating: 4.16 (scale: 0.5-5.0)\n",
      "\n",
      "Review: Mediocre at best - nothing special\n",
      "Predicted rating: 2.87 (scale: 0.5-5.0)\n",
      "\n",
      "Review: This album was the worst thing I heard in my life, Death to the artist and the producer, disgusting, awful, bad , waste of time\n",
      "Predicted rating: 3.08 (scale: 0.5-5.0)\n",
      "\n",
      "Review: Absolutely terrible album, not a single redeeming quality. I regret listening to it.\n",
      "Predicted rating: 2.22 (scale: 0.5-5.0)\n",
      "\n",
      "Review: Horrible in every way, the worst music I've ever heard.\n",
      "Predicted rating: 3.52 (scale: 0.5-5.0)\n",
      "\n",
      "Review: Unbearable noise, couldn't finish a single track.\n",
      "Predicted rating: 3.37 (scale: 0.5-5.0)\n",
      "\n",
      "Review: A complete disaster, avoid at all costs.\n",
      "Predicted rating: 0.50 (scale: 0.5-5.0)\n",
      "\n",
      "Review: Painful to listen to, a total waste of time.\n",
      "Predicted rating: 1.72 (scale: 0.5-5.0)\n",
      "\n",
      "Review: Zero talent, zero effort, zero enjoyment.\n",
      "Predicted rating: 3.93 (scale: 0.5-5.0)\n",
      "\n",
      "Review: This album is an insult to music.\n",
      "Predicted rating: 3.33 (scale: 0.5-5.0)\n",
      "\n",
      "Review: If I could give it a zero, I would.\n",
      "Predicted rating: 4.03 (scale: 0.5-5.0)\n",
      "\n",
      "Review: The most disappointing and awful release of the year.\n",
      "Predicted rating: 1.74 (scale: 0.5-5.0)\n",
      "\n",
      "Review: Disgusting, offensive, and unlistenable.\n",
      "Predicted rating: 0.50 (scale: 0.5-5.0)\n"
     ]
    }
   ],
   "source": [
    "def predict_rating(review):\n",
    "    # Preprocess\n",
    "    processed_review = preprocess_text(review)\n",
    "\n",
    "    # Transform\n",
    "    # Use Word2Vec average embedding instead of tf-idf\n",
    "    tokens = processed_review.split()\n",
    "    review_vector = get_review_vector(tokens, w2v_model, w2v_model.vector_size).reshape(1, -1)\n",
    "\n",
    "    # Predict\n",
    "    rating = ridge_w2v.predict(review_vector)[0]\n",
    "\n",
    "    # Ensure rating is within original bounds\n",
    "    min_rating, max_rating = data['Rating'].min(), data['Rating'].max()\n",
    "    rating = np.clip(rating, min_rating, max_rating)\n",
    "\n",
    "    return f\"Predicted rating: {rating:.2f} (scale: {min_rating}-{max_rating})\"\n",
    "\n",
    "# Test cases\n",
    "test_reviews = [\n",
    "    # Positive\n",
    "    \"The album was a masterpiece from start to finish\",\n",
    "    \"This album changed my life! Perfect in every way\",\n",
    "    # Mixed\n",
    "    \"Some good tracks but overall disappointing\",\n",
    "    \"The vocals were amazing, though the production quality ruined it\",\n",
    "    \"A genre-defying record that blends jazz, electronica, and rock seamlessly, though some tracks feel unnecessarily long and meandering.\",\n",
    "    \"Despite the hype, the album lacks originality and feels like a rehash of the band's previous work, with only a few standout moments.\",\n",
    "    \"The production is lush and detailed, but the lyrics are pretentious and the melodies forgettable, making for a frustrating listen.\",\n",
    "    \"There are a few catchy songs, but most of the album is forgettable.\",\n",
    "    \"The instrumentation is solid, but the songwriting leaves much to be desired.\",\n",
    "    \"Some tracks are fantastic, others are just filler.\",\n",
    "    # Sarcastic\n",
    "    \"Wow, what an album. I totally needed another hour of generic pop songs in my life.\",\n",
    "    \"Groundbreaking stuff—I've never heard such originality in a song called 'Love Tonight' before.\",\n",
    "    \"If boredom was an art form, this album would be a masterpiece.\",\n",
    "    \"Oh great, another autotuned ballad. Just what the world was missing.\",\n",
    "    \"Truly inspiring how they managed to make every track sound exactly the same.\",\n",
    "    \"I laughed, I cried, mostly because I couldn't believe I paid for this.\",\n",
    "    \"This album really redefines the word 'mediocre'.\",\n",
    "    \"So innovative, I almost didn't fall asleep halfway through.\",\n",
    "    # Negative\n",
    "    \"Mediocre at best - nothing special\",\n",
    "    \"This album was the worst thing I heard in my life, Death to the artist and the producer, disgusting, awful, bad , waste of time\",\n",
    "    \"Absolutely terrible album, not a single redeeming quality. I regret listening to it.\",\n",
    "    \"Horrible in every way, the worst music I've ever heard.\",\n",
    "    \"Unbearable noise, couldn't finish a single track.\",\n",
    "    \"A complete disaster, avoid at all costs.\",\n",
    "    \"Painful to listen to, a total waste of time.\",\n",
    "    \"Zero talent, zero effort, zero enjoyment.\",\n",
    "    \"This album is an insult to music.\",\n",
    "    \"If I could give it a zero, I would.\",\n",
    "    \"The most disappointing and awful release of the year.\",\n",
    "    \"Disgusting, offensive, and unlistenable.\"\n",
    "]\n",
    "\n",
    "for review in test_reviews:\n",
    "    print(f\"\\nReview: {review}\")\n",
    "    print(predict_rating(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e303fac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted rating for the review 'worst' is: 0.82\n"
     ]
    }
   ],
   "source": [
    "# Get user input\n",
    "text = input(\"Enter review: \")\n",
    "\n",
    "# Clean the review text\n",
    "clean_data = preprocess_text(text)  # This should return a cleaned string\n",
    "\n",
    "# Vectorize using the already trained vectorizer (do NOT use fit_transform)\n",
    "X = tfidf_vectorizer.transform([clean_data])  # Wrap in a list to avoid error\n",
    "\n",
    "# Predict using the trained model\n",
    "predicted_rating = model.predict(X)\n",
    "\n",
    "# Output the result\n",
    "print(f\"The predicted rating for the review '{text}' is: {predicted_rating[0]:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
